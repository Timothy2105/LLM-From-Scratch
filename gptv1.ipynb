{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap, random\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 16\n",
    "max_iters = 5000\n",
    "learning_rate = 3e-4\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate Vocabulary\n",
    "with open('openwebtext/char_vocab.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-Level Tokenizer\n",
    "string_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [ string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join(int_to_string[i] for i in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Map Implementation\n",
    "def get_random_chunk(split):\n",
    "    filename = \"openwebtext/train_split.txt\" if split == \"train\" else \"openwebtext/val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, file_size - batch_size*block_size)\n",
    "\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(batch_size*block_size - 1)\n",
    "\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('/r', '') # convert from bytes (return of mm) to string\n",
    "\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long) # convert to usable tensors\n",
    "\n",
    "    return data\n",
    "    \n",
    "# Input-Target Parallel Implementation\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split) # get full batch block thing\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,)) # upper bound, 1D tensor of batch_size (i.e. 32 tensors of starting points)\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"one head of attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # although same compression, weights randomly initalized -> diff results after training\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # save computation by initializing once\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input size: (B, T, C)\n",
    "        # output size: (B, T, head_size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        \n",
    "        # compute attention scores/affinities \n",
    "        # transpose(-2, -1) swaps second last dim w/ last dim, divides 1/sqrt(head_size) at end; compute (B, T, T) for token dot products\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        # self.tril here calls register_buffer that takes block_size (aka: [:T, :T], 0->T exclusive)\n",
    "        # curr: tril w/ 1 on diag and below; == 0 creates booleans that are 0, then changes True to -inf\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # changes all values above diag to 0; (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # normalize all along rows (B, T, T)\n",
    "        wei = self.dropout(wei) # apply dropout from __init__\n",
    "        \n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # ModuleList does NOT define parallel or sequential, rather the list comprehension & PyTorch does\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd) # more learnable params; note: num_heads*head_size = n_embd\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # head(x) calls the __call__ function which invokes the forward() function\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1) # combine on last dim for all heads -> (B, T, num_heads*head_size)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"simple linear layer followed by nonlinearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) # apply layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block w/ multihead-attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head # for better compute in parallel\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd) # apply linear + nonlinearity\n",
    "        self.ln1 = nn.LayerNorm(n_embd) \n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # not parallel compute\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # hidden representations -> vocab logits\n",
    "\n",
    "        self.apply(self._init_weights) # goes through all layers of neural network\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) # normal distrib\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias) # w/ underscore modifies tensor in-place\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)        \n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "\n",
    "        # B = batch size, T = tokens/sequence, C = channel/embedding dim\n",
    "        # idx and targets are (B, T) tensors\n",
    "        tok_emb = self.token_embedding_table(index) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # generate positions up to T, size (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # feed into decoding blocks, (B, T, C)\n",
    "        x = self.ln_f(x) # layer norm (B, T, C)\n",
    "        logits = self.lm_head(x) # linear to vocab (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape # v = vocab_size, probabilities for each vocab word\n",
    "            logits = logits.view(B * T, V) # compress to (B*T, V) for indiv predictions\n",
    "            targets = targets.view(B * T) # each value in (B*T) is an index for \"correct\" word\n",
    "            loss = F.cross_entropy(logits, targets) # computes softmax for each row of B*T and does -log(index of target)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # keep accurate context size\n",
    "            logits, loss = self.forward(idx_cond) # get best pred of next token\n",
    "            logits = logits[:, -1, :] # last pred, all B, last T, all V -> (B, V)\n",
    "            probs = F.softmax(logits, dim=-1) # softmax along last dim (V) -> (B, V) but normalized\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # sample one token/batch, one pred/batch -> (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # add to pred to idx -> (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0: train loss 10.4944, val loss 10.4983\n",
      "step: 500: train loss 2.2251, val loss 2.2543\n",
      "step: 1000: train loss 2.1088, val loss 2.0992\n",
      "step: 1500: train loss 2.0458, val loss 2.0427\n",
      "step: 2000: train loss 1.9942, val loss 1.9630\n",
      "step: 2500: train loss 1.9859, val loss 1.9635\n",
      "step: 3000: train loss 1.8906, val loss 1.8843\n",
      "step: 3500: train loss 1.9922, val loss 1.9795\n",
      "step: 4000: train loss 1.8998, val loss 1.8604\n",
      "step: 4500: train loss 1.8572, val loss 1.8683\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step: {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model-v1-01.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "filename = 'model-v1-01.pt'\n",
    "torch.save(model.state_dict(), filename)\n",
    "print(\"Model saved to\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the model weights/params\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "model.load_state_dict(torch.load('model_weights/model-v1-01.pt', weights_only=True))\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple iterations\n",
    "num_iterations = 5\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Training Loop\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(f'step: {iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n",
    "        \n",
    "        xb, yb = get_batch('train')\n",
    "        logits, loss = model.forward(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Save model after each full training iteration\n",
    "    filename = f'model-01-iter{iteration+1}.pt'\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "print(\"All iterations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "While is nob. Sendivey a swirth our roces. Wate.” But the emale point, countrey plose) Boy rap for autors that's misiument. In heard it procies was no amoung for likes has got on annoning is not Connecknobby, pots. 2047 came fore throughld versional continued paticing hourning in augh Her Crible than \"us there. Univers plannehing in in 1400. Chebinhe musing has make, one. It call artly -emnerian styles's News poin New potent such weall as could it was least food our comperizanise. I'm mamaging I\n"
     ]
    }
   ],
   "source": [
    "# Generate the new tokens\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu kernel",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
